{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Model Evaluation on VSI-Bench\n",
    "\n",
    "Evaluate Gemini models (2.5 Pro, 3.1 Pro) on VSI-Bench and Cambrian-S benchmarks for video spatial intelligence.\n",
    "\n",
    "## Models\n",
    "| Model | Model ID | Status |\n",
    "|-------|----------|--------|\n",
    "| Gemini 2.5 Pro | `gemini-2.5-pro-preview` | Available |\n",
    "| Gemini 3.1 Pro | `gemini-3.1-pro` | Placeholder |\n",
    "\n",
    "## Benchmarks\n",
    "- **VSI-Bench** (default): 5,000+ video spatial intelligence questions\n",
    "  - 8 task types: object direction, distance, counting, room size, etc.\n",
    "  - Metrics: Accuracy (MCA), Mean Relative Accuracy (numerical)\n",
    "- **Cambrian-S Suite** (optional): VideoMME, EgoSchema, MVBench, CV-Bench, 3DSR\n",
    "\n",
    "## Eval Modes\n",
    "- `tiny`: 50 samples (~5 min) - sanity check\n",
    "- `small`: 200 samples (~20 min) - quick evaluation\n",
    "- `full`: 5,000+ samples (~8 hours) - complete benchmark\n",
    "\n",
    "## Requirements\n",
    "- Google API key with Gemini access\n",
    "- Google Colab (any GPU tier works, API-based)\n",
    "\n",
    "## Sources\n",
    "- [VSI-Bench Paper](https://arxiv.org/abs/2412.14171)\n",
    "- [VSI-Bench Dataset](https://huggingface.co/datasets/nyu-visionx/VSI-Bench)\n",
    "- [Cambrian-S Project](https://cambrian-mllm.github.io/cambrian-s/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q google-genai>=1.0.0\n",
    "%pip install -q datasets>=2.0.0\n",
    "%pip install -q tqdm pandas matplotlib seaborn tabulate\n",
    "%pip install -q huggingface_hub\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import google.genai as genai\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"google-genai version: {genai.__version__}\")\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class EvalMode(Enum):\n",
    "    TINY = \"tiny\"      # 50 samples, ~5 min\n",
    "    SMALL = \"small\"    # 200 samples, ~20 min\n",
    "    FULL = \"full\"      # All samples\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GeminiConfig:\n",
    "    \"\"\"Configuration for Gemini API.\"\"\"\n",
    "    model_name: str = \"gemini-2.5-pro-preview\"\n",
    "    api_key: str = \"\"\n",
    "\n",
    "    # Rate limiting (conservative for free tier)\n",
    "    requests_per_minute: int = 10\n",
    "    retry_attempts: int = 3\n",
    "    retry_delay_seconds: float = 2.0\n",
    "\n",
    "    # Video settings\n",
    "    video_fps: int = 1  # Frames per second for video sampling\n",
    "    max_video_duration_seconds: int = 300  # 5 minutes max\n",
    "\n",
    "    # Generation settings\n",
    "    max_output_tokens: int = 256\n",
    "    temperature: float = 0.0  # Greedy decoding for reproducibility\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Evaluation configuration.\"\"\"\n",
    "    mode: EvalMode = EvalMode.TINY\n",
    "    seed: int = 42\n",
    "\n",
    "    # Sample counts per mode\n",
    "    TINY_SAMPLES: int = 50\n",
    "    SMALL_SAMPLES: int = 200\n",
    "\n",
    "    # Benchmark config\n",
    "    benchmark_config: str = \"full\"  # \"full\" or \"debiased\"\n",
    "\n",
    "    # Checkpoint settings\n",
    "    save_frequency: int = 10  # Save checkpoint every N samples\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> Optional[int]:\n",
    "        \"\"\"Get number of samples based on mode.\"\"\"\n",
    "        if self.mode == EvalMode.TINY:\n",
    "            return self.TINY_SAMPLES\n",
    "        elif self.mode == EvalMode.SMALL:\n",
    "            return self.SMALL_SAMPLES\n",
    "        return None  # Full benchmark\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USER CONFIGURATION - Modify these settings\n",
    "# ============================================================\n",
    "\n",
    "# API Key (get from https://aistudio.google.com/apikey)\n",
    "# Option 1: Set directly (not recommended for sharing)\n",
    "# GOOGLE_API_KEY = \"your-api-key-here\"\n",
    "\n",
    "# Option 2: Use Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', '')\n",
    "\n",
    "# Model selection\n",
    "MODELS_TO_EVALUATE = [\n",
    "    \"gemini-2.5-pro-preview\",\n",
    "    # \"gemini-3.1-pro\",  # Placeholder for future model\n",
    "]\n",
    "\n",
    "# Evaluation mode\n",
    "EVAL_MODE = \"tiny\"    # Options: \"tiny\", \"small\", \"full\"\n",
    "\n",
    "# Benchmarks to run (default: VSI-Bench only)\n",
    "BENCHMARKS_TO_RUN = [\"vsi_bench\"]\n",
    "# BENCHMARKS_TO_RUN = [\"vsi_bench\", \"videomme\", \"egoschema\"]  # Cambrian-S suite\n",
    "\n",
    "# Output directory (Google Drive)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/gemini_vsi_eval\"\n",
    "\n",
    "# Resume from checkpoint\n",
    "RESUME_FROM_CHECKPOINT = True\n",
    "\n",
    "# ============================================================\n",
    "# Create configuration objects\n",
    "# ============================================================\n",
    "\n",
    "gemini_config = GeminiConfig(\n",
    "    model_name=MODELS_TO_EVALUATE[0],\n",
    "    api_key=GOOGLE_API_KEY,\n",
    ")\n",
    "\n",
    "eval_config = EvalConfig(\n",
    "    mode=EvalMode(EVAL_MODE),\n",
    ")\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Models: {MODELS_TO_EVALUATE}\")\n",
    "print(f\"Eval mode: {eval_config.mode.value}\")\n",
    "print(f\"Samples: {eval_config.num_samples or 'all'}\")\n",
    "print(f\"Benchmarks: {BENCHMARKS_TO_RUN}\")\n",
    "print(f\"API key configured: {'Yes' if GOOGLE_API_KEY else 'No'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"\\n WARNING: No API key found!\")\n",
    "    print(\"Set GOOGLE_API_KEY in Colab secrets or environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gemini API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class GeminiClient:\n",
    "    \"\"\"Wrapper for Google Generative AI SDK with video support.\"\"\"\n",
    "\n",
    "    def __init__(self, config: GeminiConfig):\n",
    "        self.config = config\n",
    "        self._init_client()\n",
    "        self._uploaded_files: Dict[str, Any] = {}  # Cache uploaded files\n",
    "        self._last_request_time = 0.0\n",
    "        self._consecutive_errors = 0\n",
    "\n",
    "    def _init_client(self):\n",
    "        \"\"\"Initialize the Google GenAI client.\"\"\"\n",
    "        from google import genai\n",
    "        self.client = genai.Client(api_key=self.config.api_key)\n",
    "\n",
    "    def _rate_limit_wait(self):\n",
    "        \"\"\"Implement rate limiting between requests.\"\"\"\n",
    "        min_interval = 60.0 / self.config.requests_per_minute\n",
    "        elapsed = time.time() - self._last_request_time\n",
    "        if elapsed < min_interval:\n",
    "            time.sleep(min_interval - elapsed)\n",
    "        self._last_request_time = time.time()\n",
    "\n",
    "    def _handle_rate_limit_error(self):\n",
    "        \"\"\"Handle 429 error with exponential backoff.\"\"\"\n",
    "        self._consecutive_errors += 1\n",
    "        wait_time = min(2 ** self._consecutive_errors, 60)  # Max 60s\n",
    "        print(f\"Rate limited. Waiting {wait_time}s...\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "    def _reset_errors(self):\n",
    "        \"\"\"Reset error counter after successful request.\"\"\"\n",
    "        self._consecutive_errors = 0\n",
    "\n",
    "    def upload_video(self, video_path: str) -> Any:\n",
    "        \"\"\"Upload video to Gemini Files API with caching.\"\"\"\n",
    "        # Check cache first\n",
    "        cache_key = hashlib.md5(video_path.encode()).hexdigest()\n",
    "        if cache_key in self._uploaded_files:\n",
    "            file = self._uploaded_files[cache_key]\n",
    "            # Verify file still exists\n",
    "            try:\n",
    "                file = self.client.files.get(name=file.name)\n",
    "                if file.state.name == \"ACTIVE\":\n",
    "                    return file\n",
    "            except:\n",
    "                pass  # Re-upload if not found\n",
    "\n",
    "        # Upload via Files API\n",
    "        file = self.client.files.upload(file=video_path)\n",
    "\n",
    "        # Wait for processing\n",
    "        while file.state.name == \"PROCESSING\":\n",
    "            time.sleep(2)\n",
    "            file = self.client.files.get(name=file.name)\n",
    "\n",
    "        if file.state.name != \"ACTIVE\":\n",
    "            raise RuntimeError(f\"File upload failed: {file.state.name}\")\n",
    "\n",
    "        self._uploaded_files[cache_key] = file\n",
    "        return file\n",
    "\n",
    "    def generate_with_video(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        prompt: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response for video + text prompt.\"\"\"\n",
    "        self._rate_limit_wait()\n",
    "\n",
    "        for attempt in range(self.config.retry_attempts):\n",
    "            try:\n",
    "                # Upload video\n",
    "                video_file = self.upload_video(video_path)\n",
    "\n",
    "                # Generate response\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.config.model_name,\n",
    "                    contents=[video_file, prompt],\n",
    "                    config={\n",
    "                        \"temperature\": self.config.temperature,\n",
    "                        \"max_output_tokens\": self.config.max_output_tokens,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                self._reset_errors()\n",
    "                return response.text\n",
    "\n",
    "            except Exception as e:\n",
    "                error_str = str(e).lower()\n",
    "                if \"429\" in error_str or \"rate\" in error_str:\n",
    "                    self._handle_rate_limit_error()\n",
    "                elif attempt < self.config.retry_attempts - 1:\n",
    "                    time.sleep(self.config.retry_delay_seconds)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        raise RuntimeError(\"Max retries exceeded\")\n",
    "\n",
    "    def cleanup_uploaded_files(self):\n",
    "        \"\"\"Delete uploaded files to free quota.\"\"\"\n",
    "        for cache_key, file in list(self._uploaded_files.items()):\n",
    "            try:\n",
    "                self.client.files.delete(name=file.name)\n",
    "            except Exception:\n",
    "                pass\n",
    "        self._uploaded_files.clear()\n",
    "        print(f\"Cleaned up uploaded files\")\n",
    "\n",
    "\n",
    "# Test API connection\n",
    "if GOOGLE_API_KEY:\n",
    "    try:\n",
    "        client = GeminiClient(gemini_config)\n",
    "        # Quick test with text only\n",
    "        response = client.client.models.generate_content(\n",
    "            model=gemini_config.model_name,\n",
    "            contents=\"Say 'API connection successful' in exactly those words.\",\n",
    "        )\n",
    "        print(f\"API test: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VSI-Bench Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\nfrom huggingface_hub import hf_hub_download\nimport zipfile\n\n\n@dataclass\nclass VSIBenchSample:\n    \"\"\"A single VSI-Bench sample.\"\"\"\n    sample_id: str\n    video_path: str\n    question: str\n    ground_truth: str\n    options: Optional[List[str]]  # None for numerical questions\n    question_type: str  # e.g., \"object_counting\", \"room_size\"\n    task_category: str  # \"configurational\", \"measurement\", \"spatiotemporal\"\n    is_numerical: bool  # True for NA tasks, False for MCA tasks\n    metadata: Dict = field(default_factory=dict)\n\n\nclass VSIBenchLoader:\n    \"\"\"Load VSI-Bench dataset from HuggingFace.\n\n    Videos are stored separately in ZIP files and must be downloaded/extracted.\n    The dataset uses `dataset` (arkitscenes/scannet/scannetpp) and `scene_name`\n    to identify which video corresponds to each sample.\n    \"\"\"\n\n    REPO_ID = \"nyu-visionx/VSI-Bench\"\n\n    # Video sources and their ZIP files\n    VIDEO_SOURCES = [\"arkitscenes\", \"scannet\", \"scannetpp\"]\n\n    # Task category mapping\n    TASK_CATEGORIES = {\n        # Configurational tasks (MCA)\n        \"object_rel_direction\": \"configurational\",\n        \"object_rel_direction_easy\": \"configurational\",\n        \"object_rel_direction_medium\": \"configurational\",\n        \"object_rel_direction_hard\": \"configurational\",\n        \"object_rel_distance\": \"configurational\",\n        \"route_plan\": \"configurational\",\n        # Measurement estimation tasks (NA - numerical)\n        \"object_counting\": \"measurement\",\n        \"abs_dist\": \"measurement\",\n        \"object_abs_distance\": \"measurement\",\n        \"room_size\": \"measurement\",\n        \"room_size_estimation\": \"measurement\",\n        \"obj_size_estimation\": \"measurement\",\n        \"object_size_estimation\": \"measurement\",\n        # Spatiotemporal tasks (MCA)\n        \"appearance_order\": \"spatiotemporal\",\n    }\n\n    NUMERICAL_TASKS = {\n        \"object_counting\", \"abs_dist\", \"object_abs_distance\",\n        \"room_size\", \"room_size_estimation\",\n        \"obj_size_estimation\", \"object_size_estimation\"\n    }\n\n    def __init__(\n        self,\n        config: str = \"full\",  # \"full\" or \"debiased\"\n        cache_dir: Optional[str] = None,\n        num_samples: Optional[int] = None,  # Limit for testing\n        seed: int = 42,\n        stratified: bool = True,  # Ensure all task types represented\n    ):\n        self.config = config\n        self.cache_dir = Path(cache_dir or \"/content/vsi_bench_cache\")\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.video_dir = self.cache_dir / \"videos\"\n        self.video_dir.mkdir(parents=True, exist_ok=True)\n        self.num_samples = num_samples\n        self.seed = seed\n        self.stratified = stratified\n        self._dataset = None\n        self._video_paths: Dict[str, str] = {}  # (dataset, scene_name) -> video_path\n\n    def _download_and_extract_videos(self, required_sources: set):\n        \"\"\"Download and extract video ZIP files from HuggingFace.\"\"\"\n        for source in required_sources:\n            zip_name = f\"{source}.zip\"\n            extract_dir = self.video_dir / source\n\n            # Skip if already extracted\n            if extract_dir.exists() and any(extract_dir.iterdir()):\n                print(f\"  {source}: already extracted\")\n                continue\n\n            print(f\"  {source}: downloading...\")\n            try:\n                zip_path = hf_hub_download(\n                    repo_id=self.REPO_ID,\n                    filename=zip_name,\n                    repo_type=\"dataset\",\n                    cache_dir=str(self.cache_dir / \"hf_cache\"),\n                )\n\n                print(f\"  {source}: extracting...\")\n                extract_dir.mkdir(parents=True, exist_ok=True)\n                with zipfile.ZipFile(zip_path, 'r') as zf:\n                    zf.extractall(extract_dir)\n                print(f\"  {source}: done\")\n\n            except Exception as e:\n                print(f\"  {source}: failed - {e}\")\n\n    def _build_video_index(self):\n        \"\"\"Build index mapping (dataset, scene_name) to video file paths.\"\"\"\n        self._video_paths = {}\n\n        for source in self.VIDEO_SOURCES:\n            source_dir = self.video_dir / source\n\n            if not source_dir.exists():\n                continue\n\n            # Find all video files (mp4, avi, mov, etc.)\n            for video_file in source_dir.rglob(\"*\"):\n                if video_file.suffix.lower() in {\".mp4\", \".avi\", \".mov\", \".mkv\"}:\n                    # Scene name is typically the file stem or parent folder name\n                    scene_name = video_file.stem\n                    key = (source, scene_name)\n                    self._video_paths[key] = str(video_file)\n\n                    # Also try parent folder as scene name (some datasets structure it this way)\n                    if video_file.parent.name != source:\n                        alt_key = (source, video_file.parent.name)\n                        if alt_key not in self._video_paths:\n                            self._video_paths[alt_key] = str(video_file)\n\n        print(f\"Indexed {len(self._video_paths)} videos\")\n\n    def load(self) -> List[VSIBenchSample]:\n        \"\"\"Load dataset from HuggingFace.\"\"\"\n        print(f\"Loading VSI-Bench ({self.config})...\")\n\n        # Load annotations\n        self._dataset = load_dataset(\n            self.REPO_ID,\n            split=\"test\",\n        )\n\n        print(f\"Loaded {len(self._dataset)} annotation samples\")\n\n        # Apply sampling first (before downloading videos)\n        if self.num_samples and len(self._dataset) > self.num_samples:\n            if self.stratified:\n                indices = self._stratified_sample()\n            else:\n                random.seed(self.seed)\n                indices = random.sample(range(len(self._dataset)), self.num_samples)\n            self._dataset = self._dataset.select(indices)\n            print(f\"Sampled {len(self._dataset)} samples (stratified={self.stratified})\")\n\n        # Determine which video sources we need\n        required_sources = set(item[\"dataset\"] for item in self._dataset)\n        print(f\"Required video sources: {required_sources}\")\n\n        # Download and extract videos\n        print(\"Downloading videos...\")\n        self._download_and_extract_videos(required_sources)\n\n        # Build video index\n        self._build_video_index()\n\n        # Convert to samples\n        samples = []\n        missing_videos = 0\n        for idx, item in enumerate(tqdm(self._dataset, desc=\"Processing samples\")):\n            sample = self._process_item(idx, item)\n            if sample:\n                samples.append(sample)\n            else:\n                missing_videos += 1\n\n        print(f\"Processed {len(samples)} valid samples\")\n        if missing_videos > 0:\n            print(f\"Warning: {missing_videos} samples skipped (missing videos)\")\n\n        return samples\n\n    def _stratified_sample(self) -> List[int]:\n        \"\"\"Sample ensuring all task types are represented.\"\"\"\n        random.seed(self.seed)\n\n        # Group indices by task type\n        task_indices = defaultdict(list)\n        for idx, item in enumerate(self._dataset):\n            task_type = item.get(\"question_type\", \"unknown\")\n            task_indices[task_type].append(idx)\n\n        # Calculate samples per task\n        num_tasks = len(task_indices)\n        samples_per_task = max(1, self.num_samples // num_tasks)\n        remaining = self.num_samples - (samples_per_task * num_tasks)\n\n        # Sample from each task\n        selected = []\n        for task, indices in task_indices.items():\n            n = min(samples_per_task, len(indices))\n            selected.extend(random.sample(indices, n))\n\n        # Fill remaining with random samples\n        all_indices = set(range(len(self._dataset))) - set(selected)\n        if remaining > 0 and all_indices:\n            selected.extend(random.sample(list(all_indices), min(remaining, len(all_indices))))\n\n        return selected[:self.num_samples]\n\n    def _get_video_path(self, item: Dict) -> Optional[str]:\n        \"\"\"Get video path for a dataset item.\"\"\"\n        dataset = item.get(\"dataset\", \"\")\n        scene_name = item.get(\"scene_name\", \"\")\n\n        # Try exact match\n        key = (dataset, scene_name)\n        if key in self._video_paths:\n            return self._video_paths[key]\n\n        # Try without leading zeros or with different formatting\n        for (ds, sn), path in self._video_paths.items():\n            if ds == dataset and (sn == scene_name or sn.lstrip(\"0\") == scene_name.lstrip(\"0\")):\n                return path\n\n        return None\n\n    def _process_item(self, idx: int, item: Dict) -> Optional[VSIBenchSample]:\n        \"\"\"Process a single dataset item.\"\"\"\n        # Skip pruned samples if using debiased config\n        if self.config == \"debiased\" and item.get(\"pruned\", False):\n            return None\n\n        # Get video path\n        video_path = self._get_video_path(item)\n        if not video_path:\n            return None\n\n        question_type = item.get(\"question_type\", \"unknown\")\n\n        # Parse options if available\n        options = item.get(\"options\")\n        if options and isinstance(options, str):\n            options = [opt.strip() for opt in options.split(\"\\n\") if opt.strip()]\n\n        return VSIBenchSample(\n            sample_id=str(item.get(\"id\", idx)),\n            video_path=video_path,\n            question=item[\"question\"],\n            ground_truth=str(item[\"ground_truth\"]),\n            options=options,\n            question_type=question_type,\n            task_category=self.TASK_CATEGORIES.get(question_type, \"unknown\"),\n            is_numerical=question_type in self.NUMERICAL_TASKS,\n            metadata={\n                \"dataset\": item.get(\"dataset\", \"unknown\"),\n                \"scene_name\": item.get(\"scene_name\", \"\"),\n            },\n        )\n\n    def get_task_breakdown(self) -> Dict[str, int]:\n        \"\"\"Get count of samples per task type.\"\"\"\n        if self._dataset is None:\n            return {}\n        from collections import Counter\n        return dict(Counter(item[\"question_type\"] for item in self._dataset))\n\n\n# Test loader\nprint(\"Testing VSI-Bench loader...\")\ntest_loader = VSIBenchLoader(\n    config=eval_config.benchmark_config,\n    num_samples=5,  # Just test with 5\n    seed=eval_config.seed,\n)\n\ntry:\n    test_samples = test_loader.load()\n    if test_samples:\n        print(f\"\\nSample question: {test_samples[0].question[:100]}...\")\n        print(f\"Task type: {test_samples[0].question_type}\")\n        print(f\"Video path: {test_samples[0].video_path}\")\n    else:\n        print(\"\\nNo samples loaded. Videos may need to be downloaded first.\")\nexcept Exception as e:\n    print(f\"Loader test failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VSIBenchMetrics:\n",
    "    \"\"\"Evaluation metrics for VSI-Bench.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_answer(response: str, is_numerical: bool) -> Union[str, float, None]:\n",
    "        \"\"\"Extract answer from model response.\"\"\"\n",
    "        response = response.strip()\n",
    "\n",
    "        if is_numerical:\n",
    "            # Extract number from response\n",
    "            numbers = re.findall(r'[-+]?\\d*\\.?\\d+', response)\n",
    "            if numbers:\n",
    "                try:\n",
    "                    return float(numbers[0])\n",
    "                except ValueError:\n",
    "                    return None\n",
    "            return None\n",
    "        else:\n",
    "            # Extract option letter (A, B, C, D)\n",
    "            response_upper = response.upper()\n",
    "\n",
    "            # Try patterns in order of specificity\n",
    "            patterns = [\n",
    "                r'\\b([ABCD])\\b',           # Word boundary\n",
    "                r'answer[:\\s]+([ABCD])',   # \"answer: A\" pattern\n",
    "                r'^([ABCD])[\\s\\.,)]',      # Starts with letter\n",
    "                r'([ABCD])',               # Any occurrence\n",
    "            ]\n",
    "\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, response_upper)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "\n",
    "            # Fallback: first character if single letter\n",
    "            if len(response) == 1 and response.upper() in 'ABCD':\n",
    "                return response.upper()\n",
    "\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(predictions: List[str], targets: List[str]) -> float:\n",
    "        \"\"\"Calculate exact match accuracy for MCA tasks.\"\"\"\n",
    "        if not predictions:\n",
    "            return 0.0\n",
    "        correct = sum(1 for p, t in zip(predictions, targets) if p == t)\n",
    "        return correct / len(predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_relative_accuracy(\n",
    "        predictions: List[float],\n",
    "        targets: List[float],\n",
    "        start: float = 0.5,\n",
    "        end: float = 0.95,\n",
    "        interval: float = 0.05,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Mean Relative Accuracy for numerical tasks.\n",
    "\n",
    "        Based on VSI-Bench implementation:\n",
    "        https://github.com/vision-x-nyu/thinking-in-space\n",
    "        \"\"\"\n",
    "        if not predictions or not targets:\n",
    "            return 0.0\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        # Filter out invalid entries\n",
    "        valid = (targets != 0) & ~np.isnan(predictions) & ~np.isnan(targets)\n",
    "        if not valid.any():\n",
    "            return 0.0\n",
    "\n",
    "        predictions = predictions[valid]\n",
    "        targets = targets[valid]\n",
    "\n",
    "        # Normalized absolute distance\n",
    "        abs_dist_norm = np.abs(predictions - targets) / np.abs(targets)\n",
    "\n",
    "        # Calculate accuracy across confidence intervals\n",
    "        num_pts = int((end - start) / interval) + 2\n",
    "        conf_intervals = np.linspace(start, end, num_pts)\n",
    "\n",
    "        accuracies = []\n",
    "        for conf in conf_intervals:\n",
    "            acc = (abs_dist_norm <= 1 - conf).mean()\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        return float(np.mean(accuracies))\n",
    "\n",
    "    @staticmethod\n",
    "    def aggregate_by_task(\n",
    "        results: List[Dict],\n",
    "    ) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Aggregate results by task type.\"\"\"\n",
    "        task_results = defaultdict(lambda: {\n",
    "            \"mca_correct\": 0,\n",
    "            \"mca_total\": 0,\n",
    "            \"na_predictions\": [],\n",
    "            \"na_targets\": [],\n",
    "        })\n",
    "\n",
    "        for r in results:\n",
    "            if r.get(\"status\") == \"error\":\n",
    "                continue\n",
    "\n",
    "            task = r[\"question_type\"]\n",
    "            if r[\"is_numerical\"]:\n",
    "                if r[\"prediction\"] is not None and r[\"target\"] is not None:\n",
    "                    task_results[task][\"na_predictions\"].append(r[\"prediction\"])\n",
    "                    task_results[task][\"na_targets\"].append(r[\"target\"])\n",
    "            else:\n",
    "                task_results[task][\"mca_total\"] += 1\n",
    "                if r[\"prediction\"] == r[\"target\"]:\n",
    "                    task_results[task][\"mca_correct\"] += 1\n",
    "\n",
    "        # Calculate metrics per task\n",
    "        metrics = {}\n",
    "        for task, data in task_results.items():\n",
    "            if data[\"mca_total\"] > 0:\n",
    "                metrics[task] = {\n",
    "                    \"accuracy\": data[\"mca_correct\"] / data[\"mca_total\"],\n",
    "                    \"count\": data[\"mca_total\"],\n",
    "                    \"type\": \"MCA\",\n",
    "                }\n",
    "            elif data[\"na_predictions\"]:\n",
    "                mra = VSIBenchMetrics.mean_relative_accuracy(\n",
    "                    data[\"na_predictions\"],\n",
    "                    data[\"na_targets\"],\n",
    "                )\n",
    "                metrics[task] = {\n",
    "                    \"mra\": mra,\n",
    "                    \"count\": len(data[\"na_predictions\"]),\n",
    "                    \"type\": \"NA\",\n",
    "                }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Test metrics\n",
    "print(\"Testing metrics...\")\n",
    "print(f\"Extract 'A' from 'The answer is A': {VSIBenchMetrics.extract_answer('The answer is A', False)}\")\n",
    "print(f\"Extract number from '42 meters': {VSIBenchMetrics.extract_answer('42 meters', True)}\")\n",
    "print(f\"Accuracy [A,B,A] vs [A,A,A]: {VSIBenchMetrics.accuracy(['A','B','A'], ['A','A','A']):.2%}\")\n",
    "print(f\"MRA [10,20] vs [12,18]: {VSIBenchMetrics.mean_relative_accuracy([10,20], [12,18]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationCheckpoint:\n",
    "    \"\"\"Track evaluation progress for resumption.\"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    benchmark: str\n",
    "    config: str  # \"full\" or \"debiased\"\n",
    "\n",
    "    completed_ids: List[str] = field(default_factory=list)\n",
    "    results: List[Dict] = field(default_factory=list)\n",
    "\n",
    "    started_at: str = \"\"\n",
    "    last_updated: str = \"\"\n",
    "\n",
    "    # Aggregated metrics\n",
    "    overall_accuracy: float = 0.0\n",
    "    overall_mra: float = 0.0\n",
    "    task_metrics: Dict[str, Dict] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def load_or_create(\n",
    "        cls,\n",
    "        checkpoint_dir: str,\n",
    "        model_name: str,\n",
    "        benchmark: str,\n",
    "        config: str,\n",
    "    ) -> \"EvaluationCheckpoint\":\n",
    "        \"\"\"Load existing checkpoint or create new one.\"\"\"\n",
    "        # Sanitize model name for filename\n",
    "        safe_model_name = model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        checkpoint_path = Path(checkpoint_dir) / f\"{safe_model_name}_{benchmark}_{config}.json\"\n",
    "\n",
    "        if checkpoint_path.exists():\n",
    "            with open(checkpoint_path) as f:\n",
    "                data = json.load(f)\n",
    "            return cls(**data)\n",
    "\n",
    "        return cls(\n",
    "            model_name=model_name,\n",
    "            benchmark=benchmark,\n",
    "            config=config,\n",
    "            started_at=datetime.now().isoformat(),\n",
    "        )\n",
    "\n",
    "    def save(self, checkpoint_dir: str):\n",
    "        \"\"\"Save checkpoint to file.\"\"\"\n",
    "        self.last_updated = datetime.now().isoformat()\n",
    "        safe_model_name = self.model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        checkpoint_path = Path(checkpoint_dir) / f\"{safe_model_name}_{self.benchmark}_{self.config}.json\"\n",
    "        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(checkpoint_path, \"w\") as f:\n",
    "            json.dump(asdict(self), f, indent=2, default=str)\n",
    "\n",
    "    def is_completed(self, sample_id: str) -> bool:\n",
    "        \"\"\"Check if a sample has been evaluated.\"\"\"\n",
    "        return sample_id in self.completed_ids\n",
    "\n",
    "    def add_result(self, sample_id: str, result: Dict):\n",
    "        \"\"\"Add a result and mark sample as complete.\"\"\"\n",
    "        self.completed_ids.append(sample_id)\n",
    "        self.results.append(result)\n",
    "\n",
    "    def get_progress(self, total: int) -> float:\n",
    "        \"\"\"Get completion percentage.\"\"\"\n",
    "        return len(self.completed_ids) / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "class GeminiEvaluator:\n",
    "    \"\"\"Evaluate Gemini models on VSI-Bench.\"\"\"\n",
    "\n",
    "    PROMPT_TEMPLATE_MCA = \"\"\"Watch this video carefully and answer the following question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Respond with ONLY the letter of the correct answer (A, B, C, or D). Do not include any explanation.\"\"\"\n",
    "\n",
    "    PROMPT_TEMPLATE_NUMERICAL = \"\"\"Watch this video carefully and answer the following question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Respond with ONLY a single number as your answer. Do not include units or explanations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: GeminiClient,\n",
    "        checkpoint_dir: str,\n",
    "        save_frequency: int = 10,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.save_frequency = save_frequency\n",
    "\n",
    "    def format_prompt(self, sample: VSIBenchSample) -> str:\n",
    "        \"\"\"Format prompt for a sample.\"\"\"\n",
    "        if sample.is_numerical:\n",
    "            return self.PROMPT_TEMPLATE_NUMERICAL.format(\n",
    "                question=sample.question\n",
    "            )\n",
    "        else:\n",
    "            options_text = \"\\n\".join(sample.options) if sample.options else \"\"\n",
    "            return self.PROMPT_TEMPLATE_MCA.format(\n",
    "                question=sample.question,\n",
    "                options=options_text,\n",
    "            )\n",
    "\n",
    "    def evaluate_sample(self, sample: VSIBenchSample) -> Dict:\n",
    "        \"\"\"Evaluate a single sample.\"\"\"\n",
    "        prompt = self.format_prompt(sample)\n",
    "\n",
    "        try:\n",
    "            response = self.client.generate_with_video(\n",
    "                sample.video_path,\n",
    "                prompt,\n",
    "            )\n",
    "\n",
    "            # Extract answer\n",
    "            prediction = VSIBenchMetrics.extract_answer(\n",
    "                response,\n",
    "                sample.is_numerical\n",
    "            )\n",
    "\n",
    "            # Parse ground truth\n",
    "            if sample.is_numerical:\n",
    "                try:\n",
    "                    target = float(sample.ground_truth)\n",
    "                except ValueError:\n",
    "                    target = None\n",
    "            else:\n",
    "                target = sample.ground_truth.strip().upper()\n",
    "                if len(target) > 1:\n",
    "                    target = target[0]  # Take first letter\n",
    "\n",
    "            return {\n",
    "                \"sample_id\": sample.sample_id,\n",
    "                \"question_type\": sample.question_type,\n",
    "                \"task_category\": sample.task_category,\n",
    "                \"is_numerical\": sample.is_numerical,\n",
    "                \"prediction\": prediction,\n",
    "                \"target\": target,\n",
    "                \"raw_response\": response,\n",
    "                \"correct\": prediction == target if not sample.is_numerical else None,\n",
    "                \"status\": \"success\",\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"sample_id\": sample.sample_id,\n",
    "                \"question_type\": sample.question_type,\n",
    "                \"task_category\": sample.task_category,\n",
    "                \"is_numerical\": sample.is_numerical,\n",
    "                \"prediction\": None,\n",
    "                \"target\": None,\n",
    "                \"raw_response\": str(e),\n",
    "                \"correct\": False,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        samples: List[VSIBenchSample],\n",
    "        model_name: str,\n",
    "        benchmark: str = \"vsi_bench\",\n",
    "        config: str = \"full\",\n",
    "        resume: bool = True,\n",
    "    ) -> EvaluationCheckpoint:\n",
    "        \"\"\"Run evaluation on all samples.\"\"\"\n",
    "\n",
    "        # Load or create checkpoint\n",
    "        if resume:\n",
    "            checkpoint = EvaluationCheckpoint.load_or_create(\n",
    "                self.checkpoint_dir,\n",
    "                model_name,\n",
    "                benchmark,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            checkpoint = EvaluationCheckpoint(\n",
    "                model_name=model_name,\n",
    "                benchmark=benchmark,\n",
    "                config=config,\n",
    "                started_at=datetime.now().isoformat(),\n",
    "            )\n",
    "\n",
    "        # Filter to remaining samples\n",
    "        remaining = [s for s in samples if not checkpoint.is_completed(s.sample_id)]\n",
    "\n",
    "        print(f\"\\nEvaluation: {model_name}\")\n",
    "        print(f\"Total samples: {len(samples)}\")\n",
    "        print(f\"Already completed: {len(checkpoint.completed_ids)}\")\n",
    "        print(f\"Remaining: {len(remaining)}\")\n",
    "\n",
    "        if not remaining:\n",
    "            print(\"All samples already evaluated!\")\n",
    "            self._update_metrics(checkpoint)\n",
    "            return checkpoint\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(remaining, desc=f\"Evaluating\")\n",
    "\n",
    "        for i, sample in enumerate(pbar):\n",
    "            result = self.evaluate_sample(sample)\n",
    "            checkpoint.add_result(sample.sample_id, result)\n",
    "\n",
    "            # Update progress description\n",
    "            progress = checkpoint.get_progress(len(samples))\n",
    "            pbar.set_description(f\"Evaluating ({progress:.1%})\")\n",
    "\n",
    "            # Periodic save\n",
    "            if (i + 1) % self.save_frequency == 0:\n",
    "                self._update_metrics(checkpoint)\n",
    "                checkpoint.save(self.checkpoint_dir)\n",
    "\n",
    "        # Final save\n",
    "        self._update_metrics(checkpoint)\n",
    "        checkpoint.save(self.checkpoint_dir)\n",
    "\n",
    "        # Cleanup uploaded files\n",
    "        self.client.cleanup_uploaded_files()\n",
    "\n",
    "        return checkpoint\n",
    "\n",
    "    def _update_metrics(self, checkpoint: EvaluationCheckpoint):\n",
    "        \"\"\"Update aggregated metrics in checkpoint.\"\"\"\n",
    "        checkpoint.task_metrics = VSIBenchMetrics.aggregate_by_task(\n",
    "            checkpoint.results\n",
    "        )\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        mca_results = [r for r in checkpoint.results\n",
    "                       if not r[\"is_numerical\"] and r[\"status\"] == \"success\"]\n",
    "        na_results = [r for r in checkpoint.results\n",
    "                      if r[\"is_numerical\"] and r[\"status\"] == \"success\"]\n",
    "\n",
    "        if mca_results:\n",
    "            checkpoint.overall_accuracy = VSIBenchMetrics.accuracy(\n",
    "                [r[\"prediction\"] for r in mca_results],\n",
    "                [r[\"target\"] for r in mca_results],\n",
    "            )\n",
    "\n",
    "        if na_results:\n",
    "            predictions = [r[\"prediction\"] for r in na_results if r[\"prediction\"] is not None]\n",
    "            targets = [r[\"target\"] for r in na_results if r[\"target\"] is not None]\n",
    "            if predictions and targets:\n",
    "                checkpoint.overall_mra = VSIBenchMetrics.mean_relative_accuracy(\n",
    "                    predictions, targets\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Evaluation runner ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(f\"Google Drive mounted. Output dir: {OUTPUT_DIR}\")\n",
    "except:\n",
    "    print(\"Not running in Colab, using local directory\")\n",
    "    OUTPUT_DIR = \"./gemini_vsi_eval\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory ready: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VSI-Bench samples\n",
    "print(\"Loading VSI-Bench dataset...\")\n",
    "print(f\"Eval mode: {eval_config.mode.value} ({eval_config.num_samples or 'all'} samples)\")\n",
    "\n",
    "loader = VSIBenchLoader(\n",
    "    config=eval_config.benchmark_config,\n",
    "    num_samples=eval_config.num_samples,\n",
    "    seed=eval_config.seed,\n",
    "    stratified=True,\n",
    ")\n",
    "\n",
    "samples = loader.load()\n",
    "\n",
    "# Show task breakdown\n",
    "print(\"\\nTask breakdown:\")\n",
    "task_counts = defaultdict(int)\n",
    "for s in samples:\n",
    "    task_counts[s.question_type] += 1\n",
    "for task, count in sorted(task_counts.items()):\n",
    "    print(f\"  {task}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for each model\n",
    "all_results = {}\n",
    "\n",
    "for model_name in MODELS_TO_EVALUATE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create client for this model\n",
    "    model_config = GeminiConfig(\n",
    "        model_name=model_name,\n",
    "        api_key=GOOGLE_API_KEY,\n",
    "    )\n",
    "    client = GeminiClient(model_config)\n",
    "\n",
    "    # Create evaluator\n",
    "    evaluator = GeminiEvaluator(\n",
    "        client=client,\n",
    "        checkpoint_dir=OUTPUT_DIR,\n",
    "        save_frequency=eval_config.save_frequency,\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    checkpoint = evaluator.run(\n",
    "        samples=samples,\n",
    "        model_name=model_name,\n",
    "        benchmark=\"vsi_bench\",\n",
    "        config=eval_config.benchmark_config,\n",
    "        resume=RESUME_FROM_CHECKPOINT,\n",
    "    )\n",
    "\n",
    "    all_results[model_name] = checkpoint\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Overall Accuracy (MCA): {checkpoint.overall_accuracy:.2%}\")\n",
    "    print(f\"  Overall MRA (Numerical): {checkpoint.overall_mra:.3f}\")\n",
    "    print(f\"  Samples evaluated: {len(checkpoint.results)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "def create_results_dataframe(checkpoints: Dict[str, EvaluationCheckpoint]) -> pd.DataFrame:\n",
    "    \"\"\"Create DataFrame from evaluation results.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for model_name, checkpoint in checkpoints.items():\n",
    "        for task, metrics in checkpoint.task_metrics.items():\n",
    "            score = metrics.get(\"accuracy\", metrics.get(\"mra\", 0))\n",
    "            rows.append({\n",
    "                \"model\": model_name.split(\"/\")[-1],\n",
    "                \"task\": task,\n",
    "                \"score\": score,\n",
    "                \"type\": metrics.get(\"type\", \"unknown\"),\n",
    "                \"count\": metrics.get(\"count\", 0),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_task_performance(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"Create bar chart of performance by task type.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Pivot for grouped bars\n",
    "    models = df[\"model\"].unique()\n",
    "    tasks = df[\"task\"].unique()\n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.8 / len(models)\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df[\"model\"] == model]\n",
    "        scores = []\n",
    "        for task in tasks:\n",
    "            val = model_data[model_data[\"task\"] == task][\"score\"].values\n",
    "            scores.append(val[0] if len(val) > 0 else 0)\n",
    "\n",
    "        offset = (i - len(models) / 2 + 0.5) * width\n",
    "        bars = ax.bar([xi + offset for xi in x], scores, width, label=model, color=colors[i])\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, scores):\n",
    "            if score > 0:\n",
    "                ax.annotate(f\"{score:.1%}\" if score <= 1 else f\"{score:.2f}\",\n",
    "                           xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                           ha=\"center\", va=\"bottom\", fontsize=7, rotation=45)\n",
    "\n",
    "    ax.set_xlabel(\"Task\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"VSI-Bench Performance by Task\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tasks, rotation=45, ha=\"right\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_category_radar(checkpoints: Dict[str, EvaluationCheckpoint], save_path: str = None):\n",
    "    \"\"\"Create radar chart of performance by category.\"\"\"\n",
    "    categories = [\"configurational\", \"measurement\", \"spatiotemporal\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the polygon\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(checkpoints)))\n",
    "\n",
    "    for idx, (model_name, checkpoint) in enumerate(checkpoints.items()):\n",
    "        # Calculate average score per category\n",
    "        scores = []\n",
    "        for cat in categories:\n",
    "            cat_tasks = [t for t, m in checkpoint.task_metrics.items()\n",
    "                        if VSIBenchLoader.TASK_CATEGORIES.get(t) == cat]\n",
    "            if cat_tasks:\n",
    "                cat_scores = [checkpoint.task_metrics[t].get(\"accuracy\",\n",
    "                             checkpoint.task_metrics[t].get(\"mra\", 0))\n",
    "                             for t in cat_tasks]\n",
    "                scores.append(np.mean(cat_scores))\n",
    "            else:\n",
    "                scores.append(0)\n",
    "\n",
    "        scores += scores[:1]  # Close the polygon\n",
    "\n",
    "        short_name = model_name.split(\"/\")[-1]\n",
    "        ax.plot(angles, scores, \"o-\", linewidth=2, label=short_name, color=colors[idx])\n",
    "        ax.fill(angles, scores, alpha=0.1, color=colors[idx])\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([c.title() for c in categories], size=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.title(\"VSI-Bench Category Performance\", size=14, y=1.08)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Visualizations\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df = create_results_dataframe(all_results)\n",
    "\n",
    "    # Task performance chart\n",
    "    plot_task_performance(df, save_path=os.path.join(OUTPUT_DIR, \"task_performance.png\"))\n",
    "\n",
    "    # Radar chart\n",
    "    plot_category_radar(all_results, save_path=os.path.join(OUTPUT_DIR, \"category_radar.png\"))\n",
    "else:\n",
    "    print(\"No results to visualize yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "def create_leaderboard(checkpoints: Dict[str, EvaluationCheckpoint]) -> pd.DataFrame:\n",
    "    \"\"\"Create leaderboard comparing models.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for model_name, checkpoint in checkpoints.items():\n",
    "        row = {\n",
    "            \"Model\": model_name.split(\"/\")[-1],\n",
    "            \"MCA Accuracy\": f\"{checkpoint.overall_accuracy:.1%}\",\n",
    "            \"NA MRA\": f\"{checkpoint.overall_mra:.3f}\",\n",
    "            \"Samples\": len(checkpoint.results),\n",
    "        }\n",
    "\n",
    "        # Add per-category scores\n",
    "        for cat in [\"configurational\", \"measurement\", \"spatiotemporal\"]:\n",
    "            cat_tasks = [t for t, m in checkpoint.task_metrics.items()\n",
    "                        if VSIBenchLoader.TASK_CATEGORIES.get(t) == cat]\n",
    "            if cat_tasks:\n",
    "                cat_scores = [checkpoint.task_metrics[t].get(\"accuracy\",\n",
    "                             checkpoint.task_metrics[t].get(\"mra\", 0))\n",
    "                             for t in cat_tasks]\n",
    "                row[cat.title()] = f\"{np.mean(cat_scores):.1%}\"\n",
    "            else:\n",
    "                row[cat.title()] = \"N/A\"\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "if all_results:\n",
    "    leaderboard = create_leaderboard(all_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LEADERBOARD\")\n",
    "    print(\"=\" * 60)\n",
    "    print(tabulate(leaderboard, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n",
    "    # Save to CSV\n",
    "    leaderboard.to_csv(os.path.join(OUTPUT_DIR, \"leaderboard.csv\"), index=False)\n",
    "    print(f\"\\nLeaderboard saved to: {OUTPUT_DIR}/leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "def create_results_archive(output_dir: str) -> str:\n",
    "    \"\"\"Create a zip archive of all results.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"gemini_vsi_eval_{timestamp}\"\n",
    "    archive_path = f\"/content/{archive_name}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, output_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "    return archive_path\n",
    "\n",
    "\n",
    "# Create and download archive\n",
    "if all_results:\n",
    "    archive_path = create_results_archive(OUTPUT_DIR)\n",
    "    print(f\"Archive created: {archive_path}\")\n",
    "\n",
    "    # Download (in Colab)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(archive_path)\n",
    "        print(\"Download started. Check your browser's download folder.\")\n",
    "    except:\n",
    "        print(f\"Archive available at: {archive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Evaluation Complete!\n",
    "\n",
    "#### Results Summary\n",
    "- All results saved to Google Drive\n",
    "- Comparison tables and visualizations generated\n",
    "- Checkpoint saved for potential resumption\n",
    "\n",
    "#### Files Generated:\n",
    "- `*_vsi_bench_*.json` - Checkpoint files with detailed results\n",
    "- `leaderboard.csv` - Model comparison table\n",
    "- `task_performance.png` - Bar chart by task\n",
    "- `category_radar.png` - Radar chart by category\n",
    "\n",
    "#### Next Steps:\n",
    "1. Review task-level performance to identify model strengths/weaknesses\n",
    "2. Run with `EVAL_MODE = \"full\"` for complete benchmark results\n",
    "3. Add more models by updating `MODELS_TO_EVALUATE` list\n",
    "4. Enable additional Cambrian-S benchmarks via `BENCHMARKS_TO_RUN`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}