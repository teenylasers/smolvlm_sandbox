{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolVLM2 Training on Google Colab\n",
    "\n",
    "Train SmolVLM2 256M/500M vision-language models on Google Colab.\n",
    "\n",
    "**GPU Requirements:**\n",
    "- T4 (free tier): 256M model with gradient checkpointing\n",
    "- A100 (Pro/Pro+): Both 256M and 500M models\n",
    "\n",
    "**Runtime:** Go to `Runtime > Change runtime type` and select GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/YOUR_USERNAME/smolvlm_sandbox.git\n",
    "%cd smolvlm_sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q -e .\n",
    "\n",
    "# Install flash attention (optional, improves speed)\n",
    "%pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not installed (optional)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/smolvlm2_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Adjust settings based on your GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - Adjust based on your GPU\n",
    "# ============================================\n",
    "\n",
    "# Model size: \"256m\" or \"500m\"\n",
    "# - T4 (16GB): Use \"256m\"\n",
    "# - A100 (40GB): Can use \"500m\"\n",
    "MODEL_SIZE = \"256m\"\n",
    "\n",
    "# Batch size (reduce if OOM)\n",
    "# - T4: batch_size=1-2\n",
    "# - A100: batch_size=4-8\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch = BATCH_SIZE * GRAD_ACCUM\n",
    "\n",
    "# Training steps (reduce for quick test)\n",
    "MAX_STEPS = 1000  # Use 50000 for full training\n",
    "WARMUP_STEPS = 100\n",
    "SAVE_STEPS = 250\n",
    "LOGGING_STEPS = 10\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Dataset samples (reduce for quick test)\n",
    "MAX_TRAIN_SAMPLES = 10000  # Set to None for full dataset\n",
    "\n",
    "# Memory optimizations\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "USE_BF16 = True  # Use mixed precision\n",
    "\n",
    "# Weights & Biases logging (optional)\n",
    "USE_WANDB = False\n",
    "WANDB_PROJECT = \"smolvlm2-colab\"\n",
    "\n",
    "print(f\"Model: SmolVLM2-{MODEL_SIZE.upper()}\")\n",
    "print(\n",
    "    f\"Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\"\n",
    ")\n",
    "print(f\"Max steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Import from our package\n",
    "from src.model import get_config, initialize_smolvlm_model\n",
    "\n",
    "# Get config\n",
    "config = get_config(MODEL_SIZE)\n",
    "print(f\"Vision encoder: {config.vision_encoder_name}\")\n",
    "print(f\"Text decoder: {config.text_decoder_name}\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "model, processor, tokenizer = initialize_smolvlm_model(\n",
    "    model_size=MODEL_SIZE,\n",
    "    torch_dtype=torch.bfloat16 if USE_BF16 else torch.float32,\n",
    "    use_flash_attention=True,  # Falls back to eager if not available\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if USE_GRADIENT_CHECKPOINTING:\n",
    "    if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    elif hasattr(model.text_decoder, \"gradient_checkpointing_enable\"):\n",
    "        model.text_decoder.gradient_checkpointing_enable()\n",
    "    print(\"Gradient checkpointing enabled\")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import io\n\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\n# Load The Cauldron dataset (streaming to avoid downloading everything)\nprint(\"Loading dataset...\")\nraw_dataset = load_dataset(\n    \"HuggingFaceM4/the_cauldron\",\n    split=\"train\",\n    streaming=True,\n)\n\n# Take subset for testing\nif MAX_TRAIN_SAMPLES:\n    raw_dataset = raw_dataset.take(MAX_TRAIN_SAMPLES)\n    print(f\"Using {MAX_TRAIN_SAMPLES} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmolVLMDataset(Dataset):\n",
    "    \"\"\"Dataset for SmolVLM2 training.\"\"\"\n",
    "\n",
    "    def __init__(self, data, processor, tokenizer, max_length=1024):\n",
    "        self.data = list(data)  # Convert streaming dataset to list\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        print(f\"Loaded {len(self.data)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        try:\n",
    "            # Get image\n",
    "            image = None\n",
    "            if \"image\" in sample and sample[\"image\"] is not None:\n",
    "                image = sample[\"image\"]\n",
    "                if isinstance(image, bytes):\n",
    "                    image = Image.open(io.BytesIO(image))\n",
    "            elif \"images\" in sample and sample[\"images\"]:\n",
    "                image = sample[\"images\"][0]\n",
    "                if isinstance(image, bytes):\n",
    "                    image = Image.open(io.BytesIO(image))\n",
    "\n",
    "            if image is None:\n",
    "                # Return dummy sample\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "            # Ensure RGB\n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "\n",
    "            # Get text\n",
    "            if \"conversations\" in sample:\n",
    "                convs = sample[\"conversations\"]\n",
    "                text = \"\\n\".join(\n",
    "                    [f\"{c.get('from', 'user')}: {c.get('value', '')}\" for c in convs]\n",
    "                )\n",
    "            elif \"messages\" in sample:\n",
    "                msgs = sample[\"messages\"]\n",
    "                text = \"\\n\".join(\n",
    "                    [f\"{m.get('role', 'user')}: {m.get('content', '')}\" for m in msgs]\n",
    "                )\n",
    "            elif \"question\" in sample and \"answer\" in sample:\n",
    "                text = f\"user: {sample['question']}\\nassistant: {sample['answer']}\"\n",
    "            else:\n",
    "                text = str(sample.get(\"text\", \"\"))\n",
    "\n",
    "            # Add image token\n",
    "            if \"<image>\" not in text:\n",
    "                text = \"<image>\" + text\n",
    "\n",
    "            # Process image\n",
    "            pixel_values = self.processor(\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "            # Tokenize text\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"labels\": encoding[\"input_ids\"].squeeze(0).clone(),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return next sample\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "print(\"Processing dataset (this may take a few minutes)...\")\n",
    "train_dataset = SmolVLMDataset(\n",
    "    data=raw_dataset,\n",
    "    processor=processor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(f\"input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"pixel_values shape: {sample['pixel_values'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SmolVLMDataCollator:\n",
    "    \"\"\"Data collator for SmolVLM2.\"\"\"\n",
    "\n",
    "    tokenizer: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "            \"labels\": torch.stack([f[\"labels\"] for f in features]),\n",
    "            \"pixel_values\": torch.stack([f[\"pixel_values\"] for f in features]),\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = SmolVLMDataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Weights & Biases (optional)\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "\n",
    "    wandb.login()\n",
    "    report_to = [\"wandb\"]\n",
    "else:\n",
    "    report_to = [\"tensorboard\"]\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    # Batch size\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    # Learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    # Training duration\n",
    "    max_steps=MAX_STEPS,\n",
    "    # Precision\n",
    "    bf16=USE_BF16,\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    # Logging\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    report_to=report_to,\n",
    "    run_name=f\"smolvlm2-{MODEL_SIZE}-colab\",\n",
    "    # Checkpointing\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(f\"This will run for {MAX_STEPS} steps.\")\n",
    "print(\"Checkpoints will be saved to Google Drive.\\n\")\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total steps: {train_result.global_step}\")\n",
    "    print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted. Saving checkpoint...\")\n",
    "    trainer.save_model()\n",
    "    print(f\"Checkpoint saved to {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "\n",
    "print(f\"\\nModel saved to: {CHECKPOINT_DIR}\")\n",
    "print(\n",
    "    \"\\nTo download, right-click the folder in the file browser and select 'Download'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "\n",
    "# Load a test image\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "model.eval()\n",
    "\n",
    "prompt = \"<image>Describe this image in detail.\"\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Move to GPU\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.text_decoder.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resume Training (Optional)\n",
    "\n",
    "If your session disconnected, you can resume from the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import os\n",
    "\n",
    "checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "if checkpoints:\n",
    "    latest = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    resume_path = os.path.join(CHECKPOINT_DIR, latest)\n",
    "    print(f\"Found checkpoint: {resume_path}\")\n",
    "\n",
    "    # Resume training\n",
    "    # trainer.train(resume_from_checkpoint=resume_path)\n",
    "else:\n",
    "    print(\"No checkpoints found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hub\n",
    "HUB_MODEL_ID = \"your-username/smolvlm2-256m-finetuned\"  # Change this!\n",
    "\n",
    "# trainer.push_to_hub(HUB_MODEL_ID)\n",
    "# print(f\"Model pushed to: https://huggingface.co/{HUB_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tips for Colab Training\n",
    "\n",
    "### Memory Issues (OOM)\n",
    "1. Reduce `BATCH_SIZE` to 1\n",
    "2. Reduce `max_length` in dataset to 512\n",
    "3. Use 256M model instead of 500M\n",
    "4. Ensure `gradient_checkpointing` is enabled\n",
    "\n",
    "### Session Timeout\n",
    "1. Save checkpoints frequently (`SAVE_STEPS=100`)\n",
    "2. Mount Google Drive to persist checkpoints\n",
    "3. Use `resume_from_checkpoint` to continue\n",
    "\n",
    "### Speed Up Training\n",
    "1. Use Colab Pro for A100 GPU\n",
    "2. Increase `BATCH_SIZE` if memory allows\n",
    "3. Use `bf16=True` for mixed precision\n",
    "\n",
    "### Full Training\n",
    "For full 50k step training, consider:\n",
    "1. Colab Pro+ for longer sessions\n",
    "2. Cloud providers (RunPod, Lambda Labs, etc.)\n",
    "3. Multi-GPU setup with the distributed training scripts"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}