{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Model Evaluation Notebook\n",
    "\n",
    "Compare SmolVLM2 and PerceptionLM models across video and image benchmarks.\n",
    "\n",
    "## Models\n",
    "| Model | HuggingFace Path | Parameters |\n",
    "|-------|------------------|------------|\n",
    "| SmolVLM2-256M | `HuggingFaceTB/SmolVLM2-256M-Video-Instruct` | 256M |\n",
    "| SmolVLM2-500M | `HuggingFaceTB/SmolVLM2-500M-Video-Instruct` | 500M |\n",
    "| SmolVLM2-2.2B | `HuggingFaceTB/SmolVLM2-2.2B-Instruct` | 2.2B |\n",
    "| PerceptionLM-1B | `facebook/Perception-LM-1B` | 1B |\n",
    "| PerceptionLM-3B | `facebook/Perception-LM-3B` | 3B |\n",
    "\n",
    "## Benchmarks (19 total)\n",
    "- **Video (5)**: Video-MME, MLVU, MVBench, WorldSense, TempCompass\n",
    "- **Image/Document (9)**: TextVQA, DocVQA, ChartQA, MMMU, MathVista, OCRBench, AI2D, ScienceQA, MMStar\n",
    "- **PLM-VideoBench (5)**: Fine-Grained QA, Smart Glasses QA, Region Captioning, Region Temporal Localization, Region Dense Captioning\n",
    "\n",
    "## GPU Requirements\n",
    "- **T4 (Free tier)**: Works with sequential model loading and reduced batch sizes\n",
    "- **A100 (Pro)**: Faster evaluation with larger batch sizes\n",
    "\n",
    "**Runtime:** Go to `Runtime > Change runtime type` and select GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q torch torchvision torchaudio\n",
    "%pip install -q transformers>=4.40.0 accelerate>=0.27.0\n",
    "%pip install -q lmms-eval>=0.2.0\n",
    "%pip install -q decord av pillow einops safetensors\n",
    "%pip install -q pandas matplotlib seaborn tabulate tqdm\n",
    "\n",
    "# Try to install flash-attention (optional, improves performance on A100)\n",
    "%pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not installed (optional, A100 only)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository for evaluation utilities\n",
    "!git clone https://github.com/YOUR_USERNAME/smolvlm_sandbox.git 2>/dev/null || echo \"Repository already exists\"\n",
    "%cd smolvlm_sandbox\n",
    "\n",
    "# Add to path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/content/smolvlm_sandbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Detection & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class GPUTier(Enum):\n",
    "    T4_FREE = \"t4_free\"  # 15GB VRAM\n",
    "    A100_PRO = \"a100_pro\"  # 40GB VRAM\n",
    "    OTHER = \"other\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColabConfig:\n",
    "    \"\"\"Colab-specific configuration based on GPU tier.\"\"\"\n",
    "\n",
    "    gpu_tier: GPUTier\n",
    "    gpu_name: str\n",
    "    vram_gb: float\n",
    "\n",
    "    # Batch sizes per model size category\n",
    "    batch_sizes: Dict[str, int] = field(default_factory=dict)\n",
    "\n",
    "    # Memory management\n",
    "    unload_between_models: bool = True\n",
    "    unload_between_benchmarks: bool = False\n",
    "\n",
    "    # Model loading\n",
    "    device_map: str = \"auto\"\n",
    "    dtype: str = \"bfloat16\"\n",
    "    use_flash_attn: bool = False\n",
    "\n",
    "    # Evaluation settings\n",
    "    max_frames_video: int = 32\n",
    "\n",
    "    @classmethod\n",
    "    def detect(cls) -> \"ColabConfig\":\n",
    "        \"\"\"Auto-detect GPU and create appropriate config.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\n",
    "                \"No GPU available. Enable GPU in Runtime > Change runtime type\"\n",
    "            )\n",
    "\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        vram_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "        vram_gb = vram_bytes / (1024**3)\n",
    "\n",
    "        # Detect GPU tier\n",
    "        if \"T4\" in gpu_name:\n",
    "            tier = GPUTier.T4_FREE\n",
    "        elif \"A100\" in gpu_name:\n",
    "            tier = GPUTier.A100_PRO\n",
    "        else:\n",
    "            tier = GPUTier.OTHER\n",
    "\n",
    "        # Configure based on tier\n",
    "        if tier == GPUTier.T4_FREE:\n",
    "            return cls(\n",
    "                gpu_tier=tier,\n",
    "                gpu_name=gpu_name,\n",
    "                vram_gb=vram_gb,\n",
    "                batch_sizes={\n",
    "                    \"256m\": 8,\n",
    "                    \"500m\": 4,\n",
    "                    \"1b\": 2,\n",
    "                    \"2.2b\": 1,\n",
    "                    \"3b\": 1,\n",
    "                },\n",
    "                unload_between_models=True,\n",
    "                unload_between_benchmarks=True,\n",
    "                device_map=\"auto\",\n",
    "                dtype=\"bfloat16\",\n",
    "                use_flash_attn=False,\n",
    "                max_frames_video=16,\n",
    "            )\n",
    "        elif tier == GPUTier.A100_PRO:\n",
    "            return cls(\n",
    "                gpu_tier=tier,\n",
    "                gpu_name=gpu_name,\n",
    "                vram_gb=vram_gb,\n",
    "                batch_sizes={\n",
    "                    \"256m\": 32,\n",
    "                    \"500m\": 16,\n",
    "                    \"1b\": 8,\n",
    "                    \"2.2b\": 4,\n",
    "                    \"3b\": 4,\n",
    "                },\n",
    "                unload_between_models=False,\n",
    "                unload_between_benchmarks=False,\n",
    "                device_map=\"auto\",\n",
    "                dtype=\"bfloat16\",\n",
    "                use_flash_attn=True,\n",
    "                max_frames_video=32,\n",
    "            )\n",
    "        else:\n",
    "            # Conservative defaults for unknown GPUs\n",
    "            return cls(\n",
    "                gpu_tier=tier,\n",
    "                gpu_name=gpu_name,\n",
    "                vram_gb=vram_gb,\n",
    "                batch_sizes={\n",
    "                    \"256m\": 4,\n",
    "                    \"500m\": 2,\n",
    "                    \"1b\": 1,\n",
    "                    \"2.2b\": 1,\n",
    "                    \"3b\": 1,\n",
    "                },\n",
    "                unload_between_models=True,\n",
    "                unload_between_benchmarks=True,\n",
    "                device_map=\"auto\",\n",
    "                dtype=\"bfloat16\",\n",
    "                use_flash_attn=False,\n",
    "                max_frames_video=16,\n",
    "            )\n",
    "\n",
    "    def get_batch_size(self, model_size: str) -> int:\n",
    "        \"\"\"Get batch size for a model size.\"\"\"\n",
    "        size_map = {\n",
    "            \"256m\": \"256m\",\n",
    "            \"500m\": \"500m\",\n",
    "            \"1b\": \"1b\",\n",
    "            \"2.2b\": \"2.2b\",\n",
    "            \"2b\": \"2.2b\",\n",
    "            \"3b\": \"3b\",\n",
    "        }\n",
    "        normalized = size_map.get(model_size.lower(), \"1b\")\n",
    "        return self.batch_sizes.get(normalized, 1)\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Aggressively clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# Detect and display configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config = ColabConfig.detect()\n",
    "\n",
    "print(f\"GPU: {config.gpu_name}\")\n",
    "print(f\"VRAM: {config.vram_gb:.1f} GB\")\n",
    "print(f\"Tier: {config.gpu_tier.value}\")\n",
    "print(f\"Batch sizes: {config.batch_sizes}\")\n",
    "print(f\"Unload between models: {config.unload_between_models}\")\n",
    "print(f\"Max video frames: {config.max_frames_video}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Benchmark Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark definitions\n",
    "VIDEO_BENCHMARKS = [\n",
    "    \"videomme\",\n",
    "    \"mlvu\",\n",
    "    \"mvbench\",\n",
    "    \"worldsense\",\n",
    "    \"tempcompass\",\n",
    "]\n",
    "\n",
    "PLM_VIDEOBENCH = [\n",
    "    \"plm_fgqa\",\n",
    "    \"plm_sgqa\",\n",
    "    \"plm_rcap\",\n",
    "    \"plm_rtloc\",\n",
    "    \"plm_rdcap\",\n",
    "]\n",
    "\n",
    "IMAGE_BENCHMARKS = [\n",
    "    \"textvqa\",\n",
    "    \"docvqa\",\n",
    "    \"chartqa\",\n",
    "    \"mmmu_val\",\n",
    "    \"mathvista_testmini\",\n",
    "    \"ocrbench\",\n",
    "    \"ai2d\",\n",
    "    \"scienceqa_img\",\n",
    "    \"mmstar\",\n",
    "]\n",
    "\n",
    "BENCHMARK_GROUPS = {\n",
    "    \"video\": VIDEO_BENCHMARKS,\n",
    "    \"image\": IMAGE_BENCHMARKS,\n",
    "    \"plm\": PLM_VIDEOBENCH,\n",
    "    \"all\": VIDEO_BENCHMARKS + PLM_VIDEOBENCH + IMAGE_BENCHMARKS,\n",
    "}\n",
    "\n",
    "\n",
    "def resolve_benchmark_names(benchmark_str: str) -> List[str]:\n",
    "    \"\"\"Resolve benchmark string to list of task names.\"\"\"\n",
    "    if benchmark_str.lower() in BENCHMARK_GROUPS:\n",
    "        return BENCHMARK_GROUPS[benchmark_str.lower()]\n",
    "    return [b.strip() for b in benchmark_str.split(\",\")]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# USER CONFIGURATION - Modify these settings\n",
    "# ============================================\n",
    "\n",
    "# Models to evaluate (comment/uncomment to include/exclude)\n",
    "MODELS_TO_EVALUATE = [\n",
    "    # \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",  # Optional: smallest/fastest model\n",
    "    \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\",\n",
    "    # \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "    \"facebook/Perception-LM-1B\",\n",
    "    # \"facebook/Perception-LM-3B\",\n",
    "]\n",
    "\n",
    "# Benchmark selection: \"all\", \"video\", \"image\", \"plm\", or comma-separated list\n",
    "BENCHMARK_MODE = \"all\"  # Options: \"all\", \"video\", \"image\", \"plm\"\n",
    "\n",
    "# For custom selection, use:\n",
    "# BENCHMARK_MODE = \"videomme,textvqa,mmmu_val\"\n",
    "\n",
    "# Output directory (Google Drive)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/vlm_evaluation_results\"\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "RESUME_FROM_CHECKPOINT = True\n",
    "\n",
    "# ============================================\n",
    "# Resolve benchmark selection\n",
    "# ============================================\n",
    "\n",
    "benchmarks_to_run = resolve_benchmark_names(BENCHMARK_MODE)\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"Evaluation Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModels ({len(MODELS_TO_EVALUATE)}):\")\n",
    "for model in MODELS_TO_EVALUATE:\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "print(f\"\\nBenchmarks ({len(benchmarks_to_run)}):\")\n",
    "\n",
    "# Group by category\n",
    "video_selected = [b for b in benchmarks_to_run if b in VIDEO_BENCHMARKS]\n",
    "image_selected = [b for b in benchmarks_to_run if b in IMAGE_BENCHMARKS]\n",
    "plm_selected = [b for b in benchmarks_to_run if b in PLM_VIDEOBENCH]\n",
    "\n",
    "if video_selected:\n",
    "    print(f\"  Video ({len(video_selected)}): {', '.join(video_selected)}\")\n",
    "if image_selected:\n",
    "    print(f\"  Image ({len(image_selected)}): {', '.join(image_selected)}\")\n",
    "if plm_selected:\n",
    "    print(f\"  PLM ({len(plm_selected)}): {', '.join(plm_selected)}\")\n",
    "\n",
    "# Estimate time\n",
    "total_evaluations = len(MODELS_TO_EVALUATE) * len(benchmarks_to_run)\n",
    "est_time_per_eval = 10 if config.gpu_tier == GPUTier.A100_PRO else 20  # minutes\n",
    "est_total_hours = (total_evaluations * est_time_per_eval) / 60\n",
    "\n",
    "print(f\"\\nTotal evaluations: {total_evaluations}\")\n",
    "print(f\"Estimated time: {est_total_hours:.1f} hours\")\n",
    "\n",
    "if config.gpu_tier == GPUTier.T4_FREE and est_total_hours > 12:\n",
    "    print(\"\\n WARNING: This may exceed Colab free tier time limits.\")\n",
    "    print(\"Consider running in batches or using Colab Pro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mount Google Drive & Checkpoint Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Checkpoint file for resuming\n",
    "CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, \"evaluation_checkpoint.json\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationCheckpoint:\n",
    "    \"\"\"Track evaluation progress for resumption.\"\"\"\n",
    "\n",
    "    completed: List[Tuple[str, str]]  # List of (model, benchmark) tuples\n",
    "    results: Dict[str, Dict[str, Dict]]  # model -> benchmark -> metrics\n",
    "    started_at: str\n",
    "    last_updated: str\n",
    "\n",
    "    @classmethod\n",
    "    def load_or_create(cls, path: str) -> \"EvaluationCheckpoint\":\n",
    "        \"\"\"Load existing checkpoint or create new one.\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "            return cls(\n",
    "                completed=[tuple(x) for x in data.get(\"completed\", [])],\n",
    "                results=data.get(\"results\", {}),\n",
    "                started_at=data.get(\"started_at\", datetime.now().isoformat()),\n",
    "                last_updated=data.get(\"last_updated\", datetime.now().isoformat()),\n",
    "            )\n",
    "        return cls(\n",
    "            completed=[],\n",
    "            results={},\n",
    "            started_at=datetime.now().isoformat(),\n",
    "            last_updated=datetime.now().isoformat(),\n",
    "        )\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save checkpoint to file.\"\"\"\n",
    "        self.last_updated = datetime.now().isoformat()\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"completed\": self.completed,\n",
    "                    \"results\": self.results,\n",
    "                    \"started_at\": self.started_at,\n",
    "                    \"last_updated\": self.last_updated,\n",
    "                },\n",
    "                f,\n",
    "                indent=2,\n",
    "                default=str,\n",
    "            )\n",
    "\n",
    "    def mark_complete(self, model: str, benchmark: str, metrics: Dict):\n",
    "        \"\"\"Mark an evaluation as complete.\"\"\"\n",
    "        key = (model, benchmark)\n",
    "        if key not in self.completed:\n",
    "            self.completed.append(key)\n",
    "\n",
    "        if model not in self.results:\n",
    "            self.results[model] = {}\n",
    "        self.results[model][benchmark] = metrics\n",
    "\n",
    "    def is_complete(self, model: str, benchmark: str) -> bool:\n",
    "        \"\"\"Check if an evaluation is already complete.\"\"\"\n",
    "        return (model, benchmark) in self.completed\n",
    "\n",
    "    def get_remaining(\n",
    "        self, models: List[str], benchmarks: List[str]\n",
    "    ) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Get list of remaining evaluations.\"\"\"\n",
    "        all_evals = [(m, b) for m in models for b in benchmarks]\n",
    "        return [e for e in all_evals if e not in self.completed]\n",
    "\n",
    "\n",
    "# Load or create checkpoint\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    checkpoint = EvaluationCheckpoint.load_or_create(CHECKPOINT_FILE)\n",
    "    completed_count = len(checkpoint.completed)\n",
    "    if completed_count > 0:\n",
    "        print(\n",
    "            f\"Resuming from checkpoint: {completed_count} evaluations already complete\"\n",
    "        )\n",
    "else:\n",
    "    checkpoint = EvaluationCheckpoint(\n",
    "        completed=[],\n",
    "        results={},\n",
    "        started_at=datetime.now().isoformat(),\n",
    "        last_updated=datetime.now().isoformat(),\n",
    "    )\n",
    "\n",
    "remaining = checkpoint.get_remaining(MODELS_TO_EVALUATE, benchmarks_to_run)\n",
    "print(f\"Remaining evaluations: {len(remaining)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "\n",
    "class ColabEvaluationRunner:\n",
    "    \"\"\"Colab-optimized evaluation runner with memory management.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ColabConfig,\n",
    "        output_dir: str,\n",
    "        checkpoint: EvaluationCheckpoint,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir\n",
    "        self.checkpoint = checkpoint\n",
    "        self.current_model = None\n",
    "        self.current_processor = None\n",
    "        self.current_model_path = None\n",
    "\n",
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load a model, unloading previous if necessary.\"\"\"\n",
    "        if self.current_model_path == model_path:\n",
    "            return  # Already loaded\n",
    "\n",
    "        # Unload previous model\n",
    "        if self.current_model is not None:\n",
    "            print(f\"Unloading {self.current_model_path}...\")\n",
    "            del self.current_model\n",
    "            del self.current_processor\n",
    "            self.current_model = None\n",
    "            self.current_processor = None\n",
    "            self.current_model_path = None\n",
    "            clear_gpu_memory()\n",
    "\n",
    "        print(f\"Loading {model_path}...\")\n",
    "\n",
    "        # Determine settings\n",
    "        dtype = torch.bfloat16 if self.config.dtype == \"bfloat16\" else torch.float32\n",
    "        attn_impl = \"flash_attention_2\" if self.config.use_flash_attn else \"eager\"\n",
    "\n",
    "        # Load model\n",
    "        try:\n",
    "            self.current_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=self.config.device_map,\n",
    "                attn_implementation=attn_impl,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "        except Exception:\n",
    "            # Fallback without flash attention\n",
    "            self.current_model = AutoModelForImageTextToText.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=self.config.device_map,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "        self.current_model.eval()\n",
    "\n",
    "        # Load processor\n",
    "        self.current_processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        self.current_model_path = model_path\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unload current model to free memory.\"\"\"\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            del self.current_processor\n",
    "            self.current_model = None\n",
    "            self.current_processor = None\n",
    "            self.current_model_path = None\n",
    "            clear_gpu_memory()\n",
    "\n",
    "    def run_single_evaluation(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        benchmark: str,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Run a single model-benchmark evaluation using lmms-eval.\"\"\"\n",
    "\n",
    "        # Check if already complete\n",
    "        if self.checkpoint.is_complete(model_path, benchmark):\n",
    "            print(f\"  Skipping {benchmark} (already complete)\")\n",
    "            return self.checkpoint.results.get(model_path, {}).get(benchmark, {})\n",
    "\n",
    "        # Get batch size\n",
    "        model_size = self._get_model_size(model_path)\n",
    "        batch_size = self.config.get_batch_size(model_size)\n",
    "\n",
    "        # Reduce batch size for video benchmarks\n",
    "        is_video = benchmark in VIDEO_BENCHMARKS or benchmark in PLM_VIDEOBENCH\n",
    "        if is_video:\n",
    "            batch_size = max(1, batch_size // 2)\n",
    "\n",
    "        print(f\"  Running {benchmark} (batch_size={batch_size})...\")\n",
    "\n",
    "        try:\n",
    "            from lmms_eval import evaluator\n",
    "            from lmms_eval.tasks import TaskManager\n",
    "\n",
    "            # Build model args\n",
    "            model_args = f\"pretrained={model_path}\"\n",
    "            if self.config.dtype == \"bfloat16\":\n",
    "                model_args += \",dtype=bfloat16\"\n",
    "            model_args += f\",max_frames_num={self.config.max_frames_video}\"\n",
    "\n",
    "            # Initialize task manager\n",
    "            task_manager = TaskManager()\n",
    "\n",
    "            # Run evaluation\n",
    "            results = evaluator.simple_evaluate(\n",
    "                model=\"vlm\",\n",
    "                model_args=model_args,\n",
    "                tasks=[benchmark],\n",
    "                batch_size=batch_size,\n",
    "                log_samples=True,\n",
    "                task_manager=task_manager,\n",
    "            )\n",
    "\n",
    "            # Extract metrics\n",
    "            metrics = results.get(\"results\", {}).get(benchmark, {})\n",
    "\n",
    "            # Save to checkpoint\n",
    "            self.checkpoint.mark_complete(model_path, benchmark, metrics)\n",
    "            self.checkpoint.save(CHECKPOINT_FILE)\n",
    "\n",
    "            print(f\"    Success: {self._format_metrics(metrics)}\")\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Exception: {str(e)[:100]}\")\n",
    "            traceback.print_exc()\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "        finally:\n",
    "            # Clear cache after video benchmarks\n",
    "            if is_video:\n",
    "                clear_gpu_memory()\n",
    "\n",
    "    def run_all(\n",
    "        self,\n",
    "        models: List[str],\n",
    "        benchmarks: List[str],\n",
    "    ) -> Dict[str, Dict[str, Dict]]:\n",
    "        \"\"\"Run all evaluations with progress tracking.\"\"\"\n",
    "\n",
    "        remaining = self.checkpoint.get_remaining(models, benchmarks)\n",
    "        total = len(remaining)\n",
    "\n",
    "        print(f\"\\nStarting evaluation: {total} remaining\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Group by model for efficiency (minimize model reloads)\n",
    "        model_to_benchmarks = {}\n",
    "        for model, benchmark in remaining:\n",
    "            if model not in model_to_benchmarks:\n",
    "                model_to_benchmarks[model] = []\n",
    "            model_to_benchmarks[model].append(benchmark)\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(total=total, desc=\"Evaluating\")\n",
    "\n",
    "        for model_path in models:\n",
    "            if model_path not in model_to_benchmarks:\n",
    "                continue\n",
    "\n",
    "            model_benchmarks = model_to_benchmarks[model_path]\n",
    "            print(f\"\\n Model: {model_path}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            for benchmark in model_benchmarks:\n",
    "                self.run_single_evaluation(model_path, benchmark)\n",
    "                pbar.update(1)\n",
    "\n",
    "                # Unload between benchmarks if needed (T4)\n",
    "                if self.config.unload_between_benchmarks:\n",
    "                    clear_gpu_memory()\n",
    "\n",
    "            # Unload model after all benchmarks\n",
    "            if self.config.unload_between_models:\n",
    "                self.unload_model()\n",
    "\n",
    "        pbar.close()\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Evaluation complete!\")\n",
    "\n",
    "        return self.checkpoint.results\n",
    "\n",
    "    def _get_model_size(self, model_path: str) -> str:\n",
    "        \"\"\"Extract model size from path.\"\"\"\n",
    "        path_lower = model_path.lower()\n",
    "        if \"256m\" in path_lower:\n",
    "            return \"256m\"\n",
    "        elif \"500m\" in path_lower:\n",
    "            return \"500m\"\n",
    "        elif \"2.2b\" in path_lower or \"2b\" in path_lower:\n",
    "            return \"2.2b\"\n",
    "        elif \"1b\" in path_lower:\n",
    "            return \"1b\"\n",
    "        elif \"3b\" in path_lower:\n",
    "            return \"3b\"\n",
    "        return \"1b\"  # Default\n",
    "\n",
    "    def _format_metrics(self, metrics: Dict) -> str:\n",
    "        \"\"\"Format metrics for display.\"\"\"\n",
    "        if not metrics:\n",
    "            return \"No metrics\"\n",
    "\n",
    "        parts = []\n",
    "        for k, v in list(metrics.items())[:3]:\n",
    "            if isinstance(v, float):\n",
    "                parts.append(f\"{k}={v:.3f}\")\n",
    "            else:\n",
    "                parts.append(f\"{k}={v}\")\n",
    "        return \", \".join(parts)\n",
    "\n",
    "\n",
    "# Create runner\n",
    "runner = ColabEvaluationRunner(\n",
    "    config=config,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    checkpoint=checkpoint,\n",
    ")\n",
    "\n",
    "print(\"Evaluation runner ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Test (Sanity Check)\n",
    "\n",
    "Test that all models can load and generate outputs before running full benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Download test image\n",
    "TEST_IMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "\n",
    "print(\"Downloading test image...\")\n",
    "response = requests.get(TEST_IMAGE_URL)\n",
    "test_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "print(f\"Test image size: {test_image.size}\")\n",
    "\n",
    "# Display test image\n",
    "display(test_image.resize((256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference(model_path: str, image: Image.Image) -> dict:\n",
    "    \"\"\"Test inference for a single model.\"\"\"\n",
    "    result = {\n",
    "        \"model\": model_path,\n",
    "        \"status\": \"unknown\",\n",
    "        \"output\": None,\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nTesting: {model_path}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Load model\n",
    "        runner.load_model(model_path)\n",
    "\n",
    "        # Prepare prompt\n",
    "        prompt = \"Describe this image briefly.\"\n",
    "\n",
    "        # Process inputs based on model type\n",
    "        if \"smolvlm\" in model_path.lower():\n",
    "            # SmolVLM format\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}],\n",
    "                }\n",
    "            ]\n",
    "            text = runner.current_processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            inputs = runner.current_processor(\n",
    "                images=[image],\n",
    "                text=text,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(runner.current_model.device)\n",
    "        else:\n",
    "            # PerceptionLM format\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}],\n",
    "                }\n",
    "            ]\n",
    "            text = runner.current_processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            inputs = runner.current_processor(\n",
    "                images=[image],\n",
    "                text=text,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(runner.current_model.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = runner.current_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        generated_text = runner.current_processor.batch_decode(\n",
    "            outputs[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        result[\"status\"] = \"PASS\"\n",
    "        result[\"output\"] = generated_text.strip()\n",
    "\n",
    "        print(\"  Status: PASS\")\n",
    "        print(\n",
    "            f\"  Output: {result['output'][:100]}...\"\n",
    "            if len(result[\"output\"]) > 100\n",
    "            else f\"  Output: {result['output']}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"FAIL\"\n",
    "        result[\"error\"] = str(e)\n",
    "        print(\"  Status: FAIL\")\n",
    "        print(f\"  Error: {str(e)[:200]}\")\n",
    "\n",
    "    finally:\n",
    "        # Unload model\n",
    "        if config.unload_between_models:\n",
    "            runner.unload_model()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run inference tests\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE SANITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "inference_results = []\n",
    "for model_path in MODELS_TO_EVALUATE:\n",
    "    result = test_model_inference(model_path, test_image)\n",
    "    inference_results.append(result)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFERENCE TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "passed = sum(1 for r in inference_results if r[\"status\"] == \"PASS\")\n",
    "failed = sum(1 for r in inference_results if r[\"status\"] == \"FAIL\")\n",
    "\n",
    "for r in inference_results:\n",
    "    status_icon = \"OK\" if r[\"status\"] == \"PASS\" else \"X\"\n",
    "    model_name = r[\"model\"].split(\"/\")[-1]\n",
    "    print(f\"  [{status_icon}] {model_name}\")\n",
    "\n",
    "print(f\"\\nPassed: {passed}/{len(inference_results)}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(f\"\\n WARNING: {failed} model(s) failed inference test.\")\n",
    "    print(\n",
    "        \"You may want to remove failed models from MODELS_TO_EVALUATE before running benchmarks.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n All models passed inference test! Ready to run benchmarks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute Evaluation\n",
    "\n",
    "Run the full benchmark evaluation. This will automatically resume from checkpoint if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all evaluations\n",
    "# This will automatically resume from checkpoint if interrupted\n",
    "\n",
    "results = runner.run_all(\n",
    "    models=MODELS_TO_EVALUATE,\n",
    "    benchmarks=benchmarks_to_run,\n",
    ")\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Checkpoint file: {CHECKPOINT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def aggregate_results(results: Dict[str, Dict[str, Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate results into a comparison DataFrame.\"\"\"\n",
    "\n",
    "    rows = []\n",
    "    for model_path, benchmarks in results.items():\n",
    "        model_name = model_path.split(\"/\")[-1]  # Short name\n",
    "\n",
    "        for benchmark, metrics in benchmarks.items():\n",
    "            if isinstance(metrics, dict) and \"status\" not in metrics:\n",
    "                # Get primary metric\n",
    "                primary = None\n",
    "                for key in [\"accuracy\", \"acc\", \"exact_match\", \"bleu\", \"anls\"]:\n",
    "                    if key in metrics:\n",
    "                        primary = metrics[key]\n",
    "                        break\n",
    "\n",
    "                if primary is None and metrics:\n",
    "                    primary = list(metrics.values())[0]\n",
    "\n",
    "                # Determine category\n",
    "                if benchmark in VIDEO_BENCHMARKS:\n",
    "                    category = \"Video\"\n",
    "                elif benchmark in IMAGE_BENCHMARKS:\n",
    "                    category = \"Image\"\n",
    "                elif benchmark in PLM_VIDEOBENCH:\n",
    "                    category = \"PLM\"\n",
    "                else:\n",
    "                    category = \"Other\"\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"model_path\": model_path,\n",
    "                        \"benchmark\": benchmark,\n",
    "                        \"category\": category,\n",
    "                        \"score\": primary if isinstance(primary, (int, float)) else 0.0,\n",
    "                        **{\n",
    "                            k: v\n",
    "                            for k, v in metrics.items()\n",
    "                            if isinstance(v, (int, float))\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Create results DataFrame\n",
    "df = aggregate_results(checkpoint.results)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"results_summary.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to: {csv_path}\")\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Results Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results to display yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def create_comparison_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a pivot table comparing models across benchmarks.\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Pivot: benchmarks as rows, models as columns\n",
    "    pivot = df.pivot_table(\n",
    "        index=\"benchmark\",\n",
    "        columns=\"model\",\n",
    "        values=\"score\",\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "\n",
    "    # Format scores as percentages\n",
    "    pivot = pivot.applymap(\n",
    "        lambda x: f\"{x:.1%}\"\n",
    "        if pd.notna(x) and x <= 1\n",
    "        else (f\"{x:.2f}\" if pd.notna(x) else \"N/A\")\n",
    "    )\n",
    "\n",
    "    # Add benchmark category\n",
    "    def get_category(benchmark):\n",
    "        if benchmark in VIDEO_BENCHMARKS:\n",
    "            return \"Video\"\n",
    "        elif benchmark in IMAGE_BENCHMARKS:\n",
    "            return \"Image\"\n",
    "        elif benchmark in PLM_VIDEOBENCH:\n",
    "            return \"PLM\"\n",
    "        return \"Other\"\n",
    "\n",
    "    pivot[\"Category\"] = pivot.index.map(get_category)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = [\"Category\"] + [c for c in pivot.columns if c != \"Category\"]\n",
    "    pivot = pivot[cols]\n",
    "\n",
    "    # Sort by category then benchmark name\n",
    "    pivot = pivot.sort_values([\"Category\", pivot.index.name])\n",
    "\n",
    "    return pivot\n",
    "\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Create comparison table\n",
    "    comparison = create_comparison_table(df)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Model Comparison Table\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Display as formatted table\n",
    "    print(tabulate(comparison, headers=\"keys\", tablefmt=\"grid\"))\n",
    "\n",
    "    # Save to file\n",
    "    table_path = os.path.join(OUTPUT_DIR, \"comparison_table.txt\")\n",
    "    with open(table_path, \"w\") as f:\n",
    "        f.write(tabulate(comparison, headers=\"keys\", tablefmt=\"grid\"))\n",
    "    print(f\"\\nTable saved to: {table_path}\")\n",
    "else:\n",
    "    print(\"No results to display yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "def plot_benchmark_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    category: str = None,\n",
    "    figsize: tuple = (14, 6),\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"Create bar chart comparing models on benchmarks.\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    # Filter by category if specified\n",
    "    plot_df = df.copy()\n",
    "    if category:\n",
    "        plot_df = plot_df[plot_df[\"category\"] == category]\n",
    "\n",
    "    if plot_df.empty:\n",
    "        print(f\"No data for category: {category}\")\n",
    "        return\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    benchmarks = plot_df[\"benchmark\"].unique()\n",
    "    models = plot_df[\"model\"].unique()\n",
    "    x = np.arange(len(benchmarks))\n",
    "    width = 0.8 / len(models)\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = plot_df[plot_df[\"model\"] == model]\n",
    "        scores = []\n",
    "        for b in benchmarks:\n",
    "            val = model_data[model_data[\"benchmark\"] == b][\"score\"].values\n",
    "            scores.append(val[0] if len(val) > 0 else 0)\n",
    "\n",
    "        offset = (i - len(models) / 2 + 0.5) * width\n",
    "        bars = ax.bar(\n",
    "            [xi + offset for xi in x], scores, width, label=model, color=colors[i]\n",
    "        )\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, scores):\n",
    "            if score > 0:\n",
    "                label = f\"{score:.1%}\" if score <= 1 else f\"{score:.1f}\"\n",
    "                ax.annotate(\n",
    "                    label,\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=7,\n",
    "                    rotation=45,\n",
    "                )\n",
    "\n",
    "    ax.set_xlabel(\"Benchmark\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    title = f\"Model Comparison{f' - {category} Benchmarks' if category else ''}\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(benchmarks, rotation=45, ha=\"right\")\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.15, 1))\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Benchmark Comparison Charts\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Plot by category\n",
    "    for category in [\"Video\", \"Image\", \"PLM\"]:\n",
    "        category_df = df[df[\"category\"] == category]\n",
    "        if not category_df.empty:\n",
    "            plot_benchmark_comparison(\n",
    "                df,\n",
    "                category=category,\n",
    "                figsize=(12, 6),\n",
    "                save_path=os.path.join(\n",
    "                    OUTPUT_DIR, f\"comparison_{category.lower()}.png\"\n",
    "                ),\n",
    "            )\n",
    "else:\n",
    "    print(\"No results to plot yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(\n",
    "    df: pd.DataFrame,\n",
    "    figsize: tuple = (10, 10),\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"Create radar chart showing model capabilities across benchmark categories.\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    # Calculate average score per model per category\n",
    "    category_scores = (\n",
    "        df.groupby([\"model\", \"category\"])[\"score\"].mean().unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # Setup radar chart\n",
    "    categories = list(category_scores.columns)\n",
    "    N = len(categories)\n",
    "\n",
    "    if N < 3:\n",
    "        print(\"Need at least 3 categories for radar chart.\")\n",
    "        return\n",
    "\n",
    "    # Compute angle for each category\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize, subplot_kw=dict(polar=True))\n",
    "\n",
    "    # Plot each model\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(category_scores)))\n",
    "\n",
    "    for idx, (model, scores) in enumerate(category_scores.iterrows()):\n",
    "        values = scores.tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "\n",
    "        ax.plot(angles, values, \"o-\", linewidth=2, label=model, color=colors[idx])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "\n",
    "    # Set category labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=12)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "    plt.title(\"Model Capabilities Across Benchmark Categories\", size=14, y=1.08)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if len(df) > 0:\n",
    "    plot_radar_chart(df, save_path=os.path.join(OUTPUT_DIR, \"radar_chart.png\"))\n",
    "else:\n",
    "    print(\"No results to plot yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaderboard(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create ranked leaderboard of models.\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    leaderboard_rows = []\n",
    "\n",
    "    for model in df[\"model\"].unique():\n",
    "        model_df = df[df[\"model\"] == model]\n",
    "\n",
    "        row = {\n",
    "            \"Model\": model,\n",
    "            \"Overall Avg\": model_df[\"score\"].mean(),\n",
    "            \"Video Avg\": model_df[model_df[\"category\"] == \"Video\"][\"score\"].mean()\n",
    "            if \"Video\" in model_df[\"category\"].values\n",
    "            else None,\n",
    "            \"Image Avg\": model_df[model_df[\"category\"] == \"Image\"][\"score\"].mean()\n",
    "            if \"Image\" in model_df[\"category\"].values\n",
    "            else None,\n",
    "            \"PLM Avg\": model_df[model_df[\"category\"] == \"PLM\"][\"score\"].mean()\n",
    "            if \"PLM\" in model_df[\"category\"].values\n",
    "            else None,\n",
    "            \"Benchmarks\": len(model_df),\n",
    "        }\n",
    "        leaderboard_rows.append(row)\n",
    "\n",
    "    leaderboard = pd.DataFrame(leaderboard_rows)\n",
    "    leaderboard = leaderboard.sort_values(\"Overall Avg\", ascending=False)\n",
    "    leaderboard = leaderboard.reset_index(drop=True)\n",
    "    leaderboard.index = leaderboard.index + 1  # Rank starting from 1\n",
    "    leaderboard.index.name = \"Rank\"\n",
    "\n",
    "    # Format percentages\n",
    "    for col in [\"Overall Avg\", \"Video Avg\", \"Image Avg\", \"PLM Avg\"]:\n",
    "        leaderboard[col] = leaderboard[col].apply(\n",
    "            lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\"\n",
    "        )\n",
    "\n",
    "    return leaderboard\n",
    "\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Create and display leaderboard\n",
    "    leaderboard = create_leaderboard(df)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL LEADERBOARD\")\n",
    "    print(\"=\" * 60)\n",
    "    print(tabulate(leaderboard, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    # Save leaderboard\n",
    "    leaderboard_path = os.path.join(OUTPUT_DIR, \"leaderboard.csv\")\n",
    "    leaderboard.to_csv(leaderboard_path)\n",
    "    print(f\"\\nLeaderboard saved to: {leaderboard_path}\")\n",
    "else:\n",
    "    print(\"No results to display yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "def create_results_archive(output_dir: str) -> str:\n",
    "    \"\"\"Create a zip archive of all results.\"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"vlm_evaluation_results_{timestamp}\"\n",
    "    archive_path = f\"/content/{archive_name}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, output_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "    return archive_path\n",
    "\n",
    "\n",
    "# Create archive\n",
    "archive_path = create_results_archive(OUTPUT_DIR)\n",
    "print(f\"Archive created: {archive_path}\")\n",
    "\n",
    "# Download (in Colab)\n",
    "from google.colab import files\n",
    "\n",
    "files.download(archive_path)\n",
    "\n",
    "print(\"\\nDownload started. Check your browser's download folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### Evaluation Complete!\n",
    "\n",
    "#### Results Summary\n",
    "- All results saved to Google Drive\n",
    "- Comparison tables and visualizations generated\n",
    "- Checkpoint saved for potential resumption\n",
    "\n",
    "#### Files Generated:\n",
    "- `results_summary.csv` - Raw results data\n",
    "- `comparison_table.txt` - Formatted comparison table\n",
    "- `leaderboard.csv` - Ranked model leaderboard\n",
    "- `comparison_*.png` - Bar charts by category\n",
    "- `radar_chart.png` - Radar chart of capabilities\n",
    "- `evaluation_checkpoint.json` - Checkpoint for resumption\n",
    "\n",
    "#### Next Steps:\n",
    "1. Review the comparison charts to identify model strengths\n",
    "2. Use the leaderboard to select the best model for your use case\n",
    "3. Consider running additional benchmarks if needed\n",
    "4. Fine-tune promising models on your specific task"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
